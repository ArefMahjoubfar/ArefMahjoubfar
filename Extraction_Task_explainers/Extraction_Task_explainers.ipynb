{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQJOR6X1qVTKho98vdbGSQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArefMahjoubfar/ArefMahjoubfar/blob/main/Extraction_Task_explainers/Extraction_Task_explainers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfo64rdv7250"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "===============================================================================\n",
        "Project: Interpretation and Explainability for LLM Extraction using SHAP\n",
        "Author: Aref\n",
        "Part: 1 of N\n",
        "===============================================================================\n",
        "\n",
        "This module implements the foundational building blocks for an end-to-end\n",
        "experimental pipeline that:\n",
        "\n",
        "1) Loads the CT-RATE dataset from Hugging Face (CT reports with labeled fields).\n",
        "2) Asks LLMs to extract specific fields (presence, categorical, quantitative).\n",
        "3) Evaluates extraction quality with common metrics.\n",
        "4) Computes uncertainty (sample consistency, token-level probabilities)\n",
        "   and performs discrimination/calibration analyses (in Part 2).\n",
        "5) Adds SHAP explanations for token-level influence on confidence (in Part 2).\n",
        "\n",
        "This file contains:\n",
        "- Imports, logging utilities, reproducibility helpers.\n",
        "- Pydantic-based configuration models for fields, models, settings, experiment.\n",
        "- Dataset loader for CT-RATE.\n",
        "- Prompt builder for clear, JSON-only extraction prompts.\n",
        "- Hugging Face Causal LM client with token-level log-probabilities (teacher forcing).\n",
        "- Output parsing, normalization, numeric extraction, and field evaluation helpers.\n",
        "\n",
        "Installation (suggested baseline):\n",
        "    pip install datasets pandas numpy scikit-learn matplotlib seaborn sentence-transformers nltk shap transformers torch pydantic\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "# -----------------------------\n",
        "# Standard library imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "# -----------------------------\n",
        "# Third-party imports\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "# Hugging Face Transformers (for open local models, e.g., LLaMA)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Logging and reproducibility\n",
        "# =============================================================================\n",
        "\n",
        "def setup_logging(log_dir: str = \"logs\", level: int = logging.INFO) -> None:\n",
        "    \"\"\"\n",
        "    Configure Python logging to file + STDOUT.\n",
        "\n",
        "    Args:\n",
        "        log_dir: Directory where log files should be written.\n",
        "        level: Logging verbosity (e.g., logging.INFO).\n",
        "\n",
        "    Side effects:\n",
        "        - Creates the log directory if it does not exist.\n",
        "        - Sets up a new log file with a timestamp.\n",
        "        - Configures a console handler for real-time feedback.\n",
        "    \"\"\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    log_path = os.path.join(log_dir, f\"run_{time.strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "\n",
        "    logging.basicConfig(\n",
        "        level=level,\n",
        "        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
        "        handlers=[logging.FileHandler(log_path), logging.StreamHandler()],\n",
        "    )\n",
        "    logging.info(f\"Logging to {log_path}\")\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"\n",
        "    Set seeds for Python random, NumPy, and PyTorch for reproducibility.\n",
        "\n",
        "    Args:\n",
        "        seed: Integer seed to apply.\n",
        "\n",
        "    Note:\n",
        "        - Determinism is not guaranteed across all CUDA kernels.\n",
        "        - Use this to reduce variance across repeated runs.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration models (Pydantic)\n",
        "# =============================================================================\n",
        "\n",
        "class FieldSpec(BaseModel):\n",
        "    \"\"\"\n",
        "    Specification for a single field to extract from a CT report.\n",
        "\n",
        "    Types:\n",
        "        - qualitative_presence: binary detection (present/absent)\n",
        "        - quantitative_value: numeric extraction with tolerance\n",
        "        - categorical_value: normalized string category matching\n",
        "\n",
        "    Attributes:\n",
        "        name: Logical name of the field (e.g., \"pulmonary_embolism\").\n",
        "        type: One of {\"qualitative_presence\",\"quantitative_value\",\"categorical_value\"}.\n",
        "        target_column: Name of the dataset column containing the label for this field.\n",
        "        value_tolerance: For quantitative_value, absolute tolerance to accept as correct.\n",
        "        value_extractor_regex: Optional regex to capture a numeric group for extraction.\n",
        "        category_map: Optional normalization map (raw -> canonical) for categorical_value.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    type: str = Field(regex=r\"^(qualitative_presence|quantitative_value|categorical_value)$\")\n",
        "    target_column: str\n",
        "    value_tolerance: Optional[float] = None\n",
        "    value_extractor_regex: Optional[str] = None\n",
        "    category_map: Optional[Dict[str, str]] = None\n",
        "\n",
        "    @validator(\"value_tolerance\")\n",
        "    def _check_tolerance_for_quantitative(cls, v, values):\n",
        "        if values.get(\"type\") == \"quantitative_value\" and v is None:\n",
        "            raise ValueError(\"quantitative_value requires value_tolerance (absolute tolerance).\")\n",
        "        return v\n",
        "\n",
        "\n",
        "class ModelSpec(BaseModel):\n",
        "    \"\"\"\n",
        "    Model configuration and I/O behavior.\n",
        "\n",
        "    model_type:\n",
        "        - open_hf_causal_lm: Local Hugging Face Causal LM (e.g., LLaMA instruct).\n",
        "        - api_openai: Placeholder for ChatGPT endpoints (implemented later if needed).\n",
        "        - api_deepseek: Placeholder for DeepSeek R1 (implemented later if needed).\n",
        "\n",
        "    Attributes:\n",
        "        name: Short display name for reporting (e.g., \"llama3_8b_instruct\").\n",
        "        model_type: One of {\"open_hf_causal_lm\",\"api_openai\",\"api_deepseek\"}.\n",
        "        model_id: HF model id for local models (e.g., \"meta-llama/Meta-Llama-3-8B-Instruct\").\n",
        "        max_new_tokens: Upper bound on generation length.\n",
        "        top_p/top_k: Nucleus/top-k sampling controls.\n",
        "        supports_logprobs: Whether client can return token-level log-probabilities.\n",
        "        device: \"cuda\" or \"cpu\".\n",
        "        dtype: \"float16\" (default) or \"float32\" for model weights.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    model_type: str = Field(regex=r\"^(open_hf_causal_lm|api_openai|api_deepseek)$\")\n",
        "    model_id: Optional[str] = None\n",
        "    max_new_tokens: int = 128\n",
        "    top_p: Optional[float] = None\n",
        "    top_k: Optional[int] = None\n",
        "    supports_logprobs: bool = False\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dtype: str = \"float16\"\n",
        "\n",
        "\n",
        "class SettingsGrid(BaseModel):\n",
        "    \"\"\"\n",
        "    Grid of generation settings used to probe model behavior.\n",
        "\n",
        "    Attributes:\n",
        "        temperature_list: List of temperatures (e.g., [0.0, 0.5, 1.0]).\n",
        "        verbosity_list: Controls prompt style ([\"low\",\"high\"]).\n",
        "        n_consistency_samples: Repetitions for sample consistency (phase 2).\n",
        "        consistency_temperatures: Temperatures used for consistency sampling.\n",
        "    \"\"\"\n",
        "    temperature_list: List[float] = Field(default_factory=lambda: [0.0, 0.5, 1.0])\n",
        "    verbosity_list: List[str] = Field(default_factory=lambda: [\"low\", \"high\"])\n",
        "    n_consistency_samples: int = 15\n",
        "    consistency_temperatures: List[float] = Field(default_factory=lambda: [0.5, 1.0])\n",
        "\n",
        "\n",
        "class ExperimentConfig(BaseModel):\n",
        "    \"\"\"\n",
        "    Top-level configuration for a single experiment run.\n",
        "\n",
        "    Attributes:\n",
        "        dataset_name: Hugging Face dataset path/id for CT-RATE.\n",
        "        dataset_split: Which split to use (e.g., \"train\", \"validation\", \"test\").\n",
        "        text_column: Column with CT report text.\n",
        "        fields: List of FieldSpec describing the extraction targets.\n",
        "        models: List of ModelSpec for the models we will evaluate.\n",
        "        settings: SettingsGrid for temperature/verbosity/consistency loops.\n",
        "        output_dir: Where to write artifacts (CSVs, plots, SHAP exports).\n",
        "        seed: Global seed for reproducibility.\n",
        "        human_consistency_file: Optional CSV with human agreement for sample consistency.\n",
        "        embedding_model: Sentence-embedding model name used in phase 2 (provided later).\n",
        "    \"\"\"\n",
        "    dataset_name: str = \"CT-RATE\"  # replace if the HF path differs (e.g., \"org/ct-rate\")\n",
        "    dataset_split: str = \"train\"\n",
        "    text_column: str = \"text\"\n",
        "    fields: List[FieldSpec]\n",
        "    models: List[ModelSpec]\n",
        "    settings: SettingsGrid\n",
        "    output_dir: str = \"artifacts\"\n",
        "    seed: int = 42\n",
        "    human_consistency_file: Optional[str] = None\n",
        "    embedding_model: str = \"intfloat/e5-small-v2\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Dataset loader (CT-RATE)\n",
        "# =============================================================================\n",
        "\n",
        "class CTRateDataset:\n",
        "    \"\"\"\n",
        "    Wrapper around the CT-RATE dataset from Hugging Face.\n",
        "\n",
        "    Expected schema:\n",
        "        - text_column contains the CT report paragraph (default \"text\").\n",
        "        - For each FieldSpec.target_column, a label column exists in the dataset.\n",
        "          For qualitative_presence, the label is expected to be 0/1.\n",
        "          For quantitative/categorical, adapt or create appropriate target columns.\n",
        "\n",
        "    Usage:\n",
        "        ds = CTRateDataset(cfg)\n",
        "        ds.load()\n",
        "        for sample in ds.iter_samples():\n",
        "            ...  # sample = {\"id\": int, \"text\": str, \"labels\": {field_name: label}}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: ExperimentConfig):\n",
        "        self.cfg = cfg\n",
        "        self.ds = None\n",
        "        self.df = None\n",
        "\n",
        "    def load(self) -> None:\n",
        "        \"\"\"\n",
        "        Load dataset into memory and enforce presence of required columns.\n",
        "        \"\"\"\n",
        "        logging.info(f\"Loading dataset: {self.cfg.dataset_name} [{self.cfg.dataset_split}]\")\n",
        "        self.ds = load_dataset(self.cfg.dataset_name, split=self.cfg.dataset_split)\n",
        "        self.df = self.ds.to_pandas()\n",
        "\n",
        "        # Check the text column exists\n",
        "        assert self.cfg.text_column in self.df.columns, f\"Missing text column: {self.cfg.text_column}\"\n",
        "\n",
        "        # Check all target columns exist\n",
        "        for f in self.cfg.fields:\n",
        "            assert f.target_column in self.df.columns, f\"Missing target column: {f.target_column}\"\n",
        "\n",
        "        logging.info(f\"Dataset loaded with shape: {self.df.shape}\")\n",
        "\n",
        "    def iter_samples(self):\n",
        "        \"\"\"\n",
        "        Iterate samples as a lightweight generator.\n",
        "\n",
        "        Yields:\n",
        "            dict with:\n",
        "                - id: integer row index\n",
        "                - text: CT report paragraph\n",
        "                - labels: mapping of field_name -> label (from target_column)\n",
        "        \"\"\"\n",
        "        for i, row in self.df.iterrows():\n",
        "            yield {\n",
        "                \"id\": i,\n",
        "                \"text\": row[self.cfg.text_column],\n",
        "                \"labels\": {f.name: row[f.target_column] for f in self.cfg.fields},\n",
        "            }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Prompt builder\n",
        "# =============================================================================\n",
        "\n",
        "class PromptBuilder:\n",
        "    \"\"\"\n",
        "    Builds consistent prompts for field extraction.\n",
        "\n",
        "    Verbosity modes:\n",
        "        - low: concise, direct JSON-only answer, no explanations.\n",
        "        - high: allows internal reasoning but must still return JSON-only at the end.\n",
        "\n",
        "    The schema we show the model is small and explicit to encourage structured output.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def make_schema(field: FieldSpec) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a JSON schema snippet for the requested field.\n",
        "\n",
        "        Returns:\n",
        "            A minimal schema for the model to follow.\n",
        "        \"\"\"\n",
        "        if field.type == \"qualitative_presence\":\n",
        "            return {\"field_name\": field.name, \"present\": \"boolean\"}\n",
        "        elif field.type == \"quantitative_value\":\n",
        "            return {\"field_name\": field.name, \"present\": \"boolean\", \"value\": \"number\"}\n",
        "        elif field.type == \"categorical_value\":\n",
        "            return {\"field_name\": field.name, \"present\": \"boolean\", \"value\": \"string\"}\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported field type: {field.type}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def prompt_text(field: FieldSpec, text: str, verbosity: str = \"low\") -> str:\n",
        "        \"\"\"\n",
        "        Construct a prompt that asks the model to output ONLY a JSON object.\n",
        "\n",
        "        Args:\n",
        "            field: FieldSpec describing what to extract.\n",
        "            text: CT report paragraph.\n",
        "            verbosity: \"low\" or \"high\" affecting style guidance.\n",
        "\n",
        "        Returns:\n",
        "            A formatted prompt string.\n",
        "        \"\"\"\n",
        "        schema = PromptBuilder.make_schema(field)\n",
        "        schema_str = json.dumps(schema, indent=2)\n",
        "\n",
        "        base_instr = (\n",
        "            \"You are an information extraction assistant for radiology CT reports. \"\n",
        "            \"Extract the requested field from the report. Return ONLY a valid JSON object \"\n",
        "            \"matching the given schema, with no additional text.\"\n",
        "        )\n",
        "        style_low = \"Be concise. Do not add explanations.\"\n",
        "        style_high = \"Think carefully. You may reason internally, but return ONLY the final JSON.\"\n",
        "\n",
        "        style = style_low if verbosity == \"low\" else style_high\n",
        "\n",
        "        return (\n",
        "            f\"{base_instr}\\n\"\n",
        "            f\"{style}\\n\\n\"\n",
        "            f\"Report:\\n{text}\\n\\n\"\n",
        "            f\"Field to extract: {field.name}\\n\"\n",
        "            f\"Output JSON schema:\\n{schema_str}\\n\"\n",
        "            f\"Return ONLY JSON.\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Model clients\n",
        "# =============================================================================\n",
        "\n",
        "class BaseModelClient:\n",
        "    \"\"\"\n",
        "    Abstract interface for model clients.\n",
        "\n",
        "    Subclasses should implement:\n",
        "        - generate(): produce text given a prompt and decoding settings.\n",
        "        - get_token_logprobs(): optional per-token log-probabilities if supported.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spec: ModelSpec):\n",
        "        self.spec = spec\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        temperature: float = 0.0,\n",
        "        top_p: Optional[float] = None,\n",
        "        top_k: Optional[int] = None,\n",
        "        max_new_tokens: Optional[int] = None,\n",
        "        seed: Optional[int] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a text completion.\n",
        "\n",
        "        Returns:\n",
        "            dict with keys:\n",
        "                - text: generated continuation (ideally JSON per our prompt)\n",
        "                - raw: full decoded sequence (prompt + continuation), when available\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_token_logprobs(self, text: str) -> Optional[List[float]]:\n",
        "        \"\"\"\n",
        "        Compute per-token log probabilities for a given text (teacher-forced).\n",
        "        Return None if not supported by the client.\n",
        "\n",
        "        Note:\n",
        "            This is useful as a proxy for confidence (to be calibrated later).\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "\n",
        "class HFOpenCausalLMClient(BaseModelClient):\n",
        "    \"\"\"\n",
        "    Hugging Face Causal LM client (e.g., latest LLaMA Instruct).\n",
        "\n",
        "    - Loads tokenizer and model weights locally.\n",
        "    - Supports generation with temperature/top_p/top_k controls.\n",
        "    - Provides token-level log-probabilities for an arbitrary text via teacher forcing.\n",
        "\n",
        "    Caution:\n",
        "        - Teacher-forced log-probs reflect how plausible the sequence is under the model,\n",
        "          not strictly the model's generation-time sampling probabilities (but they correlate).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spec: ModelSpec):\n",
        "        super().__init__(spec)\n",
        "        assert spec.model_id, \"HF client requires model_id for open_hf_causal_lm\"\n",
        "        logging.info(f\"Loading HF model: {spec.model_id} on device {spec.device}\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(spec.model_id)\n",
        "        dtype = torch.float16 if spec.dtype == \"float16\" else torch.float32\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            spec.model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=\"auto\" if spec.device == \"cuda\" else None\n",
        "        )\n",
        "        self.model.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        temperature: float = 0.0,\n",
        "        top_p: Optional[float] = None,\n",
        "        top_k: Optional[int] = None,\n",
        "        max_new_tokens: Optional[int] = None,\n",
        "        seed: Optional[int] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a continuation for the given prompt.\n",
        "\n",
        "        Implementation details:\n",
        "            - Uses do_sample=True when temperature > 0, otherwise greedy-ish (temperature clamped).\n",
        "            - Decodes output and returns only the continuation (not the prompt).\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            set_seed(seed)\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        gen_kwargs = dict(\n",
        "            temperature=max(0.01, float(temperature)),  # avoid exact 0 for some samplers\n",
        "            do_sample=temperature > 0,\n",
        "            max_new_tokens=max_new_tokens or self.spec.max_new_tokens,\n",
        "        )\n",
        "        if top_p is not None:\n",
        "            gen_kwargs[\"top_p\"] = top_p\n",
        "        if top_k is not None:\n",
        "            gen_kwargs[\"top_k\"] = top_k\n",
        "\n",
        "        output_ids = self.model.generate(**inputs, **gen_kwargs)\n",
        "        full_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract continuation by trimming the prompt portion\n",
        "        prompt_text = self.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "        continuation = full_text[len(prompt_text):]\n",
        "\n",
        "        return {\"text\": continuation.strip(), \"raw\": full_text}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_token_logprobs(self, text: str) -> Optional[List[float]]:\n",
        "        \"\"\"\n",
        "        Compute per-token log-probabilities for an arbitrary text using teacher forcing.\n",
        "\n",
        "        Steps:\n",
        "            - Tokenize the full text.\n",
        "            - Forward with labels to get next-token logits.\n",
        "            - Compute log-softmax, then gather log-prob of each gold next token.\n",
        "            - Returns a list of length (len(tokens)-1), each the log-prob for token t given tokens < t.\n",
        "\n",
        "        Returns:\n",
        "            List[float] of per-token log-probs, or None on failure.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            enc = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
        "            outputs = self.model(**enc, labels=enc[\"input_ids\"])  # labels trigger loss computation\n",
        "            logits = outputs.logits[:, :-1, :]          # predict token t+1 from positions up to t\n",
        "            labels = enc[\"input_ids\"][:, 1:]            # gold next tokens\n",
        "\n",
        "            logprobs = torch.log_softmax(logits, dim=-1)             # [B, T-1, V]\n",
        "            token_logprobs = logprobs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # [B, T-1]\n",
        "\n",
        "            return token_logprobs.flatten().tolist()\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"HF get_token_logprobs failed: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Parsing and evaluation helpers\n",
        "# =============================================================================\n",
        "\n",
        "class ExtractedRecord(BaseModel):\n",
        "    \"\"\"\n",
        "    Parsed model output for a single field extraction.\n",
        "\n",
        "    Attributes:\n",
        "        field_name: Name of the field this JSON refers to.\n",
        "        present: True/False indicating whether the field is present in the paragraph.\n",
        "        value: Optional numeric/string value (quantitative/categorical).\n",
        "    \"\"\"\n",
        "    field_name: str\n",
        "    present: bool\n",
        "    value: Optional[Union[float, str]] = None\n",
        "\n",
        "\n",
        "def parse_json_output(raw_text: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse a JSON object from model output.\n",
        "\n",
        "    Behavior:\n",
        "        - Attempts to find the first {...} block (robust against pre/post text).\n",
        "        - If found, loads that as JSON; otherwise tries to parse entire string as JSON.\n",
        "        - Returns None on failure.\n",
        "\n",
        "    Args:\n",
        "        raw_text: The raw text produced by the model.\n",
        "\n",
        "    Returns:\n",
        "        Dict representing the JSON object, or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        m = re.search(r\"\\{.*\\}\", raw_text, flags=re.DOTALL)\n",
        "        if m:\n",
        "            return json.loads(m.group(0))\n",
        "        return json.loads(raw_text)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_categorical(value: Optional[str], category_map: Optional[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Normalize categorical string values for robust comparison.\n",
        "\n",
        "    Strategy:\n",
        "        - Lowercase + strip whitespace.\n",
        "        - If category_map is provided and contains the key, map to the canonical form.\n",
        "        - Otherwise return the lowercased value.\n",
        "\n",
        "    Args:\n",
        "        value: Raw predicted or gold categorical label.\n",
        "        category_map: Optional normalization dictionary.\n",
        "\n",
        "    Returns:\n",
        "        Normalized string (possibly empty if input is None).\n",
        "    \"\"\"\n",
        "    if value is None:\n",
        "        return \"\"\n",
        "    v = str(value).strip().lower()\n",
        "    if category_map and v in category_map:\n",
        "        return category_map[v]\n",
        "    return v\n",
        "\n",
        "\n",
        "def extract_numeric(text: str, regex: Optional[str] = None) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Extract a numeric value from a text string.\n",
        "\n",
        "    Behavior:\n",
        "        - If regex is provided, use it and return the first captured float group.\n",
        "        - Otherwise, fallback to the first number-like token found.\n",
        "\n",
        "    Args:\n",
        "        text: Source string to search.\n",
        "        regex: Optional capturing regex (must capture the numeric in group 1).\n",
        "\n",
        "    Returns:\n",
        "        float or None if no numeric can be parsed.\n",
        "    \"\"\"\n",
        "    if regex:\n",
        "        m = re.search(regex, text)\n",
        "        if m:\n",
        "            try:\n",
        "                return float(m.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # Fallback: first number-like token\n",
        "    m2 = re.search(r\"[-+]?\\d*\\.?\\d+\", text)\n",
        "    if m2:\n",
        "        try:\n",
        "            return float(m2.group(0))\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def evaluate_field(\n",
        "    field: FieldSpec,\n",
        "    gold: Any,\n",
        "    pred: ExtractedRecord\n",
        ") -> Tuple[int, float]:\n",
        "    \"\"\"\n",
        "    Evaluate correctness for a single field prediction.\n",
        "\n",
        "    Returns:\n",
        "        (correct_binary, crude_score)\n",
        "        - correct_binary: 1 if correct, else 0.\n",
        "        - crude_score: a simple [0,1] score used in phase 1 (for ROC-AUC, etc.);\n",
        "                       phase 2 adds calibrated uncertainty -> probability of correctness.\n",
        "\n",
        "    Logic by type:\n",
        "        - qualitative_presence:\n",
        "            correct if bool(gold) == pred.present.\n",
        "        - quantitative_value:\n",
        "            correct if pred.present is True AND gold is numeric AND |pred.value - gold| <= tolerance.\n",
        "            If gold is None and pred.present is False => correct (no value expected).\n",
        "        - categorical_value:\n",
        "            correct if pred.present is True AND normalized(pred.value) == normalized(gold).\n",
        "            If no gold category and pred.present is False => correct.\n",
        "    \"\"\"\n",
        "    if field.type == \"qualitative_presence\":\n",
        "        correct = int(bool(gold) == bool(pred.present))\n",
        "        return correct, float(correct)\n",
        "\n",
        "    elif field.type == \"quantitative_value\":\n",
        "        if pred.present and gold is not None and isinstance(gold, (int, float)) and pred.value is not None:\n",
        "            try:\n",
        "                pred_val = float(pred.value)\n",
        "                tol = field.value_tolerance or 0.0\n",
        "                correct = int(abs(pred_val - float(gold)) <= tol)\n",
        "                return correct, float(correct)\n",
        "            except Exception:\n",
        "                return 0, 0.0\n",
        "        # Both absent => correct\n",
        "        if (gold is None) and (not pred.present):\n",
        "            return 1, 1.0\n",
        "        return 0, 0.0\n",
        "\n",
        "    elif field.type == \"categorical_value\":\n",
        "        gold_norm = normalize_categorical(str(gold), field.category_map) if gold is not None else \"\"\n",
        "        pred_norm = normalize_categorical(str(pred.value), field.category_map) if pred.value is not None else \"\"\n",
        "\n",
        "        if gold_norm and pred.present:\n",
        "            correct = int(gold_norm == pred_norm)\n",
        "            return correct, float(correct)\n",
        "\n",
        "        if (not gold_norm) and (not pred.present):\n",
        "            return 1, 1.0\n",
        "\n",
        "        return 0, 0.0\n",
        "\n",
        "    # Unknown type (should never happen due to validation)\n",
        "    return 0, 0.0\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    y_true: List[int],\n",
        "    y_pred: List[int],\n",
        "    y_score: Optional[List[float]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compute standard binary classification metrics.\n",
        "\n",
        "    Metrics:\n",
        "        - confusion_matrix: TN, FP, FN, TP\n",
        "        - accuracy, precision, recall, f1\n",
        "        - sensitivity (== recall)\n",
        "        - specificity = TN / (TN + FP)\n",
        "        - roc_auc (if y_score provided and valid)\n",
        "\n",
        "    Args:\n",
        "        y_true: Ground truth binary labels.\n",
        "        y_pred: Binary predictions.\n",
        "        y_score: Optional probability-like scores for ROC-AUC.\n",
        "\n",
        "    Returns:\n",
        "        Dict with all computed metrics. ROC-AUC may be None if not computable.\n",
        "    \"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "    out = {\n",
        "        \"confusion_matrix\": {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)},\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"specificity\": specificity,\n",
        "        \"sensitivity\": sensitivity,\n",
        "        \"roc_auc\": None\n",
        "    }\n",
        "\n",
        "    if y_score is not None:\n",
        "        # ROC-AUC can fail (e.g., only one class present)\n",
        "        try:\n",
        "            from sklearn.metrics import roc_auc_score\n",
        "            out[\"roc_auc\"] = float(roc_auc_score(y_true, y_score))\n",
        "        except Exception:\n",
        "            out[\"roc_auc\"] = None\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Part 2: Uncertainty scoring\n",
        "# =============================================================================\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from numpy.linalg import norm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "class ConsistencyResult(BaseModel):\n",
        "    \"\"\"\n",
        "    Stores computed consistency statistics across multiple generations for the same prompt.\n",
        "    \"\"\"\n",
        "    samples: List[str]\n",
        "    agreement_fraction: float\n",
        "    embedding_cosine_mean: float\n",
        "    bleu_mean: float\n",
        "    human_agreement_fraction: Optional[float] = None\n",
        "\n",
        "\n",
        "class UncertaintyScorer:\n",
        "    \"\"\"\n",
        "    Computes:\n",
        "        - Sample Consistency metrics:\n",
        "            * Majority answer agreement fraction\n",
        "            * Mean embedding cosine similarity\n",
        "            * Mean BLEU score\n",
        "            * Optional human annotation agreement\n",
        "        - Token-level probability metrics:\n",
        "            * Average token probability\n",
        "            * Minimum token probability\n",
        "        - Calibration diagnostics:\n",
        "            * Expected Calibration Error (ECE)\n",
        "            * Brier Score\n",
        "            * Calibration plots\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: ExperimentConfig):\n",
        "        self.cfg = cfg\n",
        "        self.embedder = SentenceTransformer(cfg.embedding_model)\n",
        "        self.bleu_smooth = SmoothingFunction().method1\n",
        "        self.human_map = None\n",
        "        if cfg.human_consistency_file and os.path.exists(cfg.human_consistency_file):\n",
        "            df = pd.read_csv(cfg.human_consistency_file)\n",
        "            self.human_map = {\n",
        "                int(r[\"sample_id\"]): float(r[\"agree_count\"]) / float(r[\"total\"]) if r[\"total\"] > 0 else None\n",
        "                for _, r in df.iterrows()\n",
        "            }\n",
        "\n",
        "    @staticmethod\n",
        "    def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "        return float(np.dot(a, b) / (norm(a) * norm(b) + 1e-8))\n",
        "\n",
        "    def sample_consistency(\n",
        "        self,\n",
        "        client: BaseModelClient,\n",
        "        prompt: str,\n",
        "        n_samples: int = 15,\n",
        "        temperature: float = 0.7,\n",
        "        seed_base: int = 1000,\n",
        "        sample_id: Optional[int] = None,\n",
        "    ) -> ConsistencyResult:\n",
        "        \"\"\"\n",
        "        Generate multiple outputs for the same prompt and compute agreement metrics.\n",
        "        \"\"\"\n",
        "        gens = []\n",
        "        for i in range(n_samples):\n",
        "            res = client.generate(prompt, temperature=temperature, seed=seed_base + i)\n",
        "            gens.append(res[\"text\"].strip())\n",
        "\n",
        "        # Majority agreement\n",
        "        norm_gens = [re.sub(r\"\\s+\", \" \", g.lower()).strip() for g in gens]\n",
        "        counts = pd.Series(norm_gens).value_counts()\n",
        "        agreement_fraction = counts.iloc[0] / max(1, len(norm_gens))\n",
        "\n",
        "        # Embedding cosine similarities (mean of all pairs)\n",
        "        embs = self.embedder.encode(gens, convert_to_numpy=True, normalize_embeddings=True)\n",
        "        sims = [self.cosine_sim(embs[i], embs[j])\n",
        "                for i in range(len(embs)) for j in range(i + 1, len(embs))]\n",
        "        emb_cos_mean = float(np.mean(sims)) if sims else 1.0\n",
        "\n",
        "        # BLEU mean pairwise\n",
        "        bleu_scores = [sentence_bleu([gens[i].split()], gens[j].split(),\n",
        "                                     smoothing_function=self.bleu_smooth)\n",
        "                       for i in range(len(gens)) for j in range(i + 1, len(gens))]\n",
        "        bleu_mean = float(np.mean(bleu_scores)) if bleu_scores else 1.0\n",
        "\n",
        "        human_frac = None\n",
        "        if self.human_map and sample_id is not None:\n",
        "            human_frac = self.human_map.get(sample_id)\n",
        "\n",
        "        return ConsistencyResult(\n",
        "            samples=gens,\n",
        "            agreement_fraction=agreement_fraction,\n",
        "            embedding_cosine_mean=emb_cos_mean,\n",
        "            bleu_mean=bleu_mean,\n",
        "            human_agreement_fraction=human_frac\n",
        "        )\n",
        "\n",
        "    def token_level_probabilities(self, client: BaseModelClient, text: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute average and minimum token probabilities for a given text.\n",
        "        \"\"\"\n",
        "        logs = client.get_token_logprobs(text)\n",
        "        out = {\"avg_token_prob\": np.nan, \"min_token_prob\": np.nan}\n",
        "        if logs is None or len(logs) == 0:\n",
        "            return out\n",
        "        probs = np.exp(np.array(logs, dtype=np.float64))\n",
        "        out[\"avg_token_prob\"] = float(np.mean(probs))\n",
        "        out[\"min_token_prob\"] = float(np.min(probs))\n",
        "        return out\n",
        "\n",
        "    # ---- Calibration helpers ----\n",
        "    @staticmethod\n",
        "    def brier_score(y_true: List[int], p_pred: List[float]) -> float:\n",
        "        y, p = np.array(y_true), np.array(p_pred)\n",
        "        return float(np.mean((p - y) ** 2))\n",
        "\n",
        "    @staticmethod\n",
        "    def expected_calibration_error(y_true: List[int], p_pred: List[float], n_bins: int = 10) -> float:\n",
        "        y, p = np.array(y_true), np.array(p_pred)\n",
        "        bins = np.linspace(0, 1, n_bins + 1)\n",
        "        ece = 0.0\n",
        "        for i in range(n_bins):\n",
        "            mask = (p >= bins[i]) & (p < bins[i + 1] if i < n_bins - 1 else p <= bins[i + 1])\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "            acc = np.mean(y[mask])\n",
        "            conf = np.mean(p[mask])\n",
        "            ece += np.mean(mask) * abs(acc - conf)\n",
        "        return float(ece)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_calibration(y_true: List[int], p_pred: List[float], title: str, out_path: str, n_bins: int = 10) -> None:\n",
        "        \"\"\"\n",
        "        Save a calibration plot comparing predicted confidence vs. observed accuracy.\n",
        "        \"\"\"\n",
        "        y, p = np.array(y_true), np.array(p_pred)\n",
        "        bins = np.linspace(0, 1, n_bins + 1)\n",
        "        xs, ys = [], []\n",
        "        for i in range(n_bins):\n",
        "            mask = (p >= bins[i]) & (p < bins[i + 1] if i < n_bins - 1 else p <= bins[i + 1])\n",
        "            if not np.any(mask):\n",
        "                continue\n",
        "            xs.append(np.mean(p[mask]))\n",
        "            ys.append(np.mean(y[mask]))\n",
        "        plt.figure()\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label=\"Perfect calibration\")\n",
        "        plt.scatter(xs, ys, c='b')\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Predicted probability\")\n",
        "        plt.ylabel(\"Observed accuracy\")\n",
        "        plt.legend()\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        plt.savefig(out_path)\n",
        "        plt.close()\n",
        "\n",
        "    def calibrate_metric_to_prob(self, metric_values: List[float], correct_labels: List[int]) -> callable:\n",
        "        \"\"\"\n",
        "        Fit an isotonic regression mapping from metric to P(correct).\n",
        "        \"\"\"\n",
        "        from sklearn.isotonic import IsotonicRegression\n",
        "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "        iso.fit(np.array(metric_values), np.array(correct_labels))\n",
        "        return lambda vals: iso.predict(np.array(vals))\n"
      ],
      "metadata": {
        "id": "PkfF9pAT8l2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Part 3: SHAP Explainer\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import warnings\n",
        "\n",
        "class SHAPExplainer:\n",
        "    \"\"\"\n",
        "    Provides tools to apply SHAP to our extraction pipeline.\n",
        "\n",
        "    Main modes:\n",
        "        1) Token-level influence on a confidence score:\n",
        "            - Wrap a model call and scoring function into f(texts) -> confidences.\n",
        "            - Use shap.maskers.Text and KernelExplainer to see token contributions.\n",
        "        2) Selecting \"interesting\" cases for explanation:\n",
        "            - Based on calibrated probability-of-correctness (near decision boundary).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Quiet down SHAP's verbose logging/warnings\n",
        "        shap.logger.setLevel(logging.ERROR)\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_text_to_confidence_fn(\n",
        "        client: BaseModelClient,\n",
        "        field: FieldSpec,\n",
        "        verbosity: str,\n",
        "        temperature_for_score: float,\n",
        "        score_extractor: callable,\n",
        "    ) -> callable:\n",
        "        \"\"\"\n",
        "        Create a function f(texts) -> confidences for SHAP.\n",
        "\n",
        "        This function reâ€‘runs the model on each input text, extracts a scalar\n",
        "        confidence using the provided score_extractor, and clamps it to [0,1].\n",
        "        \"\"\"\n",
        "\n",
        "        def f(texts: List[str]) -> np.ndarray:\n",
        "            scores = []\n",
        "            for t in texts:\n",
        "                prompt = PromptBuilder.prompt_text(field, t, verbosity=verbosity)\n",
        "                res = client.generate(prompt, temperature=temperature_for_score)\n",
        "                score = score_extractor(res[\"text\"])\n",
        "                scores.append(float(np.clip(score, 0.0, 1.0)))\n",
        "            return np.array(scores, dtype=np.float64)\n",
        "\n",
        "        return f\n",
        "\n",
        "    def explain_token_influence(\n",
        "        self,\n",
        "        client: BaseModelClient,\n",
        "        field: FieldSpec,\n",
        "        text: str,\n",
        "        verbosity: str,\n",
        "        temperature_for_score: float,\n",
        "        score_extractor: callable,\n",
        "        max_evals: int = 256,\n",
        "        seed: int = 123,\n",
        "        plot: bool = True,\n",
        "        out_path: Optional[str] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run SHAP KernelExplainer to attribute each token's contribution to the\n",
        "        confidence score for a single text sample.\n",
        "\n",
        "        Args:\n",
        "            client: Model client (must implement generate()).\n",
        "            field: FieldSpec for which we are explaining the extraction.\n",
        "            text: The CT report paragraph to explain.\n",
        "            verbosity: Prompt verbosity (\"low\"/\"high\").\n",
        "            temperature_for_score: Generation temperature for scoring run.\n",
        "            score_extractor: Callable that maps model output -> confidence in [0,1].\n",
        "            max_evals: Max SHAP kernel samples.\n",
        "            seed: RNG seed for SHAP.\n",
        "            plot: Whether to generate a matplotlib plot.\n",
        "            out_path: Optional file path to save the plot.\n",
        "\n",
        "        Returns:\n",
        "            Dict containing SHAP values for the single example.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        f = self.make_text_to_confidence_fn(client, field, verbosity,\n",
        "                                            temperature_for_score, score_extractor)\n",
        "        masker = shap.maskers.Text(tokenizer=None)  # uses default whitespace splitting\n",
        "\n",
        "        explainer = shap.KernelExplainer(f, masker)\n",
        "        shap_values = explainer.shap_values([text], nsamples=max_evals, silent=True)\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(10, 3))\n",
        "            shap.plots.text(shap_values)\n",
        "            if out_path:\n",
        "                os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "                plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "                plt.close()\n",
        "\n",
        "        return {\"shap_values\": shap_values}\n",
        "\n",
        "    @staticmethod\n",
        "    def select_cases_for_explanation(\n",
        "        p_correct: List[float],\n",
        "        ids: List[Any],\n",
        "        low_thresh: float = 0.4,\n",
        "        high_thresh: float = 0.6,\n",
        "        top_k: int = 20\n",
        "    ) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Select cases with predicted probability-of-correctness near the decision\n",
        "        boundary (e.g., between 0.4 and 0.6) for closer inspection.\n",
        "\n",
        "        Returns:\n",
        "            List of sample IDs chosen.\n",
        "        \"\"\"\n",
        "        idxs = [i for i, p in enumerate(p_correct) if low_thresh <= p <= high_thresh]\n",
        "        idxs_sorted = sorted(idxs, key=lambda i: abs(p_correct[i] - 0.5))\n",
        "        return [ids[i] for i in idxs_sorted[:top_k]]\n"
      ],
      "metadata": {
        "id": "b-OyZ6Rw8xDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Part 3: SHAP Explainer\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import warnings\n",
        "\n",
        "class SHAPExplainer:\n",
        "    \"\"\"\n",
        "    Provides tools to apply SHAP to our extraction pipeline.\n",
        "\n",
        "    Main modes:\n",
        "        1) Token-level influence on a confidence score:\n",
        "            - Wrap a model call and scoring function into f(texts) -> confidences.\n",
        "            - Use shap.maskers.Text and KernelExplainer to see token contributions.\n",
        "        2) Selecting \"interesting\" cases for explanation:\n",
        "            - Based on calibrated probability-of-correctness (near decision boundary).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Quiet down SHAP's verbose logging/warnings\n",
        "        shap.logger.setLevel(logging.ERROR)\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_text_to_confidence_fn(\n",
        "        client: BaseModelClient,\n",
        "        field: FieldSpec,\n",
        "        verbosity: str,\n",
        "        temperature_for_score: float,\n",
        "        score_extractor: callable,\n",
        "    ) -> callable:\n",
        "        \"\"\"\n",
        "        Create a function f(texts) -> confidences for SHAP.\n",
        "\n",
        "        This function reâ€‘runs the model on each input text, extracts a scalar\n",
        "        confidence using the provided score_extractor, and clamps it to [0,1].\n",
        "        \"\"\"\n",
        "\n",
        "        def f(texts: List[str]) -> np.ndarray:\n",
        "            scores = []\n",
        "            for t in texts:\n",
        "                prompt = PromptBuilder.prompt_text(field, t, verbosity=verbosity)\n",
        "                res = client.generate(prompt, temperature=temperature_for_score)\n",
        "                score = score_extractor(res[\"text\"])\n",
        "                scores.append(float(np.clip(score, 0.0, 1.0)))\n",
        "            return np.array(scores, dtype=np.float64)\n",
        "\n",
        "        return f\n",
        "\n",
        "    def explain_token_influence(\n",
        "        self,\n",
        "        client: BaseModelClient,\n",
        "        field: FieldSpec,\n",
        "        text: str,\n",
        "        verbosity: str,\n",
        "        temperature_for_score: float,\n",
        "        score_extractor: callable,\n",
        "        max_evals: int = 256,\n",
        "        seed: int = 123,\n",
        "        plot: bool = True,\n",
        "        out_path: Optional[str] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run SHAP KernelExplainer to attribute each token's contribution to the\n",
        "        confidence score for a single text sample.\n",
        "\n",
        "        Args:\n",
        "            client: Model client (must implement generate()).\n",
        "            field: FieldSpec for which we are explaining the extraction.\n",
        "            text: The CT report paragraph to explain.\n",
        "            verbosity: Prompt verbosity (\"low\"/\"high\").\n",
        "            temperature_for_score: Generation temperature for scoring run.\n",
        "            score_extractor: Callable that maps model output -> confidence in [0,1].\n",
        "            max_evals: Max SHAP kernel samples.\n",
        "            seed: RNG seed for SHAP.\n",
        "            plot: Whether to generate a matplotlib plot.\n",
        "            out_path: Optional file path to save the plot.\n",
        "\n",
        "        Returns:\n",
        "            Dict containing SHAP values for the single example.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        f = self.make_text_to_confidence_fn(client, field, verbosity,\n",
        "                                            temperature_for_score, score_extractor)\n",
        "        masker = shap.maskers.Text(tokenizer=None)  # uses default whitespace splitting\n",
        "\n",
        "        explainer = shap.KernelExplainer(f, masker)\n",
        "        shap_values = explainer.shap_values([text], nsamples=max_evals, silent=True)\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(10, 3))\n",
        "            shap.plots.text(shap_values)\n",
        "            if out_path:\n",
        "                os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "                plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "                plt.close()\n",
        "\n",
        "        return {\"shap_values\": shap_values}\n",
        "\n",
        "    @staticmethod\n",
        "    def select_cases_for_explanation(\n",
        "        p_correct: List[float],\n",
        "        ids: List[Any],\n",
        "        low_thresh: float = 0.4,\n",
        "        high_thresh: float = 0.6,\n",
        "        top_k: int = 20\n",
        "    ) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Select cases with predicted probability-of-correctness near the decision\n",
        "        boundary (e.g., between 0.4 and 0.6) for closer inspection.\n",
        "\n",
        "        Returns:\n",
        "            List of sample IDs chosen.\n",
        "        \"\"\"\n",
        "        idxs = [i for i, p in enumerate(p_correct) if low_thresh <= p <= high_thresh]\n",
        "        idxs_sorted = sorted(idxs, key=lambda i: abs(p_correct[i] - 0.5))\n",
        "        return [ids[i] for i in idxs_sorted[:top_k]]\n"
      ],
      "metadata": {
        "id": "W4OKYqNz8z5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Part 4: ExperimentRunner orchestration\n",
        "# =============================================================================\n",
        "\n",
        "import pathlib\n",
        "import re\n",
        "\n",
        "def safe_roc_auc(y_true: List[int], scores: List[float]) -> Optional[float]:\n",
        "    \"\"\"\n",
        "    Safe wrapper for roc_auc_score:\n",
        "        - Returns None if only one class present or scores have no variance.\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    y = np.array(y_true, dtype=int)\n",
        "    s = np.array(scores, dtype=float)\n",
        "    if len(np.unique(y)) < 2 or np.allclose(np.std(s), 0.0):\n",
        "        return None\n",
        "    try:\n",
        "        return float(roc_auc_score(y, s))\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "class ExperimentRunner:\n",
        "    \"\"\"\n",
        "    Coordinates all phases:\n",
        "        1) Phase 1: Extraction & evaluation (per-sample + summary metrics)\n",
        "        2) Phase 2: Uncertainty computation (sample consistency & token-level probs),\n",
        "           plus discrimination & calibration.\n",
        "        3) Phase 3: SHAP explanation on selected borderline-confidence cases.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: ExperimentConfig):\n",
        "        self.cfg = cfg\n",
        "        self.ds = CTRateDataset(cfg)\n",
        "        self.uncertainty = UncertaintyScorer(cfg)\n",
        "        self.shapx = SHAPExplainer()\n",
        "        os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "    def run(self):\n",
        "        set_seed(self.cfg.seed)\n",
        "        self.ds.load()\n",
        "\n",
        "        # Accumulators for later export\n",
        "        rows_eval = []             # Phase 1: per-sample\n",
        "        rows_summary = []          # Phase 1: per-field summary\n",
        "        rows_uncert = []           # Phase 2: per-sample uncertainty values\n",
        "        rows_uncert_summary = []   # Phase 2: per-field/metric summary\n",
        "\n",
        "        # Loop over models and settings\n",
        "        for model_spec in self.cfg.models:\n",
        "            logging.info(f\"Starting model: {model_spec.name}\")\n",
        "            client = HFOpenCausalLMClient(model_spec)  # For now: HF client only\n",
        "\n",
        "            for temperature in self.cfg.settings.temperature_list:\n",
        "                for verbosity in self.cfg.settings.verbosity_list:\n",
        "                    logging.info(f\"Settings: T={temperature}, Verbosity={verbosity}\")\n",
        "\n",
        "                    # Per-field containers for Phase 1 metrics\n",
        "                    y_true_map, y_pred_map, y_score_map, ids_map = {}, {}, {}, {}\n",
        "                    metric_store = {}\n",
        "\n",
        "                    for field in self.cfg.fields:\n",
        "                        y_true_map[field.name] = []\n",
        "                        y_pred_map[field.name] = []\n",
        "                        y_score_map[field.name] = []\n",
        "                        ids_map[field.name] = []\n",
        "                        metric_store[field.name] = {\"tlp\": []}\n",
        "\n",
        "                    # ===== Phase 1: One extraction per sample =====\n",
        "                    for sample in self.ds.iter_samples():\n",
        "                        sid, text, labels = sample[\"id\"], sample[\"text\"], sample[\"labels\"]\n",
        "\n",
        "                        for field in self.cfg.fields:\n",
        "                            prompt = PromptBuilder.prompt_text(field, text, verbosity)\n",
        "                            res = client.generate(prompt, temperature=temperature,\n",
        "                                                  top_p=model_spec.top_p, top_k=model_spec.top_k,\n",
        "                                                  max_new_tokens=model_spec.max_new_tokens,\n",
        "                                                  seed=self.cfg.seed + sid)\n",
        "                            raw = res[\"text\"]\n",
        "                            parsed = parse_json_output(raw) or {}\n",
        "                            pred = ExtractedRecord(field_name=parsed.get(\"field_name\", field.name),\n",
        "                                                   present=bool(parsed.get(\"present\", False)),\n",
        "                                                   value=parsed.get(\"value\", None))\n",
        "\n",
        "                            gold = labels[field.name]\n",
        "                            correct, score = evaluate_field(field, gold, pred)\n",
        "                            pred_bin = int(bool(pred.present)) if field.type == \"qualitative_presence\" else int(correct)\n",
        "\n",
        "                            y_true_map[field.name].append(int(bool(gold)) if field.type == \"qualitative_presence\" else int(correct))\n",
        "                            y_pred_map[field.name].append(pred_bin)\n",
        "                            y_score_map[field.name].append(score)\n",
        "                            ids_map[field.name].append(sid)\n",
        "\n",
        "                            # Token-level probs\n",
        "                            tlp = self.uncertainty.token_level_probabilities(client, raw)\n",
        "                            metric_store[field.name][\"tlp\"].append({\"sample_id\": sid,\n",
        "                                                                    \"avg_token_prob\": tlp[\"avg_token_prob\"],\n",
        "                                                                    \"min_token_prob\": tlp[\"min_token_prob\"]})\n",
        "\n",
        "                            rows_eval.append({\n",
        "                                \"model\": model_spec.name,\n",
        "                                \"temperature\": temperature,\n",
        "                                \"verbosity\": verbosity,\n",
        "                                \"sample_id\": sid,\n",
        "                                \"field\": field.name,\n",
        "                                \"gold\": gold,\n",
        "                                \"pred_present\": pred.present,\n",
        "                                \"pred_value\": pred.value,\n",
        "                                \"correct\": correct,\n",
        "                                \"score\": score,\n",
        "                                \"raw_output\": raw\n",
        "                            })\n",
        "\n",
        "                    # Summarize Phase 1\n",
        "                    for field in self.cfg.fields:\n",
        "                        metrics = compute_metrics(y_true_map[field.name],\n",
        "                                                  y_pred_map[field.name],\n",
        "                                                  y_score_map[field.name])\n",
        "                        rows_summary.append({\"model\": model_spec.name,\n",
        "                                             \"temperature\": temperature,\n",
        "                                             \"verbosity\": verbosity,\n",
        "                                             \"field\": field.name,\n",
        "                                             **metrics})\n",
        "\n",
        "                    # ===== Phase 2: Sample consistency per field/sample =====\n",
        "                    for field in self.cfg.fields:\n",
        "                        ids_subset = ids_map[field.name][:100]  # cap for cost\n",
        "                        for t_cons in self.cfg.settings.consistency_temperatures:\n",
        "                            for sid in ids_subset:\n",
        "                                text = self.ds.df.loc[sid, self.cfg.text_column]\n",
        "                                prompt = PromptBuilder.prompt_text(field, text, verbosity)\n",
        "                                cres = self.uncertainty.sample_consistency(client, prompt,\n",
        "                                                                           n_samples=self.cfg.settings.n_consistency_samples,\n",
        "                                                                           temperature=t_cons,\n",
        "                                                                           seed_base=self.cfg.seed + 999,\n",
        "                                                                           sample_id=sid)\n",
        "                                rows_uncert.append({\n",
        "                                    \"model\": model_spec.name,\n",
        "                                    \"temperature\": temperature,\n",
        "                                    \"verbosity\": verbosity,\n",
        "                                    \"field\": field.name,\n",
        "                                    \"sample_id\": sid,\n",
        "                                    \"consistency_temp\": t_cons,\n",
        "                                    \"agreement_fraction\": cres.agreement_fraction,\n",
        "                                    \"embedding_cosine_mean\": cres.embedding_cosine_mean,\n",
        "                                    \"bleu_mean\": cres.bleu_mean,\n",
        "                                    \"human_agreement_fraction\": cres.human_agreement_fraction\n",
        "                                })\n",
        "\n",
        "                    # ===== Phase 2: Discrimination + calibration =====\n",
        "                    df_eval = pd.DataFrame(rows_eval)\n",
        "                    df_unc = pd.DataFrame(rows_uncert)\n",
        "\n",
        "                    for field in self.cfg.fields:\n",
        "                        df_field = df_eval[(df_eval[\"model\"] == model_spec.name) &\n",
        "                                           (df_eval[\"temperature\"] == temperature) &\n",
        "                                           (df_eval[\"verbosity\"] == verbosity) &\n",
        "                                           (df_eval[\"field\"] == field.name)].copy()\n",
        "                        # Attach token-level probs\n",
        "                        df_tlp = pd.DataFrame(metric_store[field.name][\"tlp\"])\n",
        "                        df_field = df_field.merge(df_tlp, on=\"sample_id\", how=\"left\")\n",
        "\n",
        "                        # Check each uncertainty metric\n",
        "                        for metric_name in [\"avg_token_prob\", \"min_token_prob\"]:\n",
        "                            dfm = df_field.dropna(subset=[metric_name, \"correct\"])\n",
        "                            if dfm.empty: continue\n",
        "                            y_true = dfm[\"correct\"].astype(int).tolist()\n",
        "                            vals = dfm[metric_name].astype(float).tolist()\n",
        "                            auc = safe_roc_auc(y_true, vals)\n",
        "                            calibrator = self.uncertainty.calibrate_metric_to_prob(vals, y_true)\n",
        "                            p_hat = calibrator(vals)\n",
        "                            ece = self.uncertainty.expected_calibration_error(y_true, p_hat)\n",
        "                            brier = self.uncertainty.brier_score(y_true, p_hat)\n",
        "\n",
        "                            plot_path = os.path.join(self.cfg.output_dir, \"calibration_plots\",\n",
        "                                                     f\"{model_spec.name}_{field.name}_{metric_name}_T{temperature}_V{verbosity}.png\")\n",
        "                            self.uncertainty.plot_calibration(y_true, p_hat, f\"{metric_name} calibration\", plot_path)\n",
        "\n",
        "                            rows_uncert_summary.append({\n",
        "                                \"model\": model_spec.name, \"temperature\": temperature, \"verbosity\": verbosity,\n",
        "                                \"field\": field.name, \"metric\": metric_name, \"consistency_temp\": None,\n",
        "                                \"roc_auc\": auc, \"ece\": ece, \"brier\": brier, \"n\": len(y_true),\n",
        "                                \"calibration_plot\": plot_path\n",
        "                            })\n",
        "\n",
        "        # ===== Save all outputs =====\n",
        "        outdir = pathlib.Path(self.cfg.output_dir)\n",
        "        outdir.mkdir(parents=True, exist_ok=True)\n",
        "        pd.DataFrame(rows_eval).to_csv(outdir / \"phase1_per_sample.csv\", index=False)\n",
        "        pd.DataFrame(rows_summary).to_csv(outdir / \"phase1_summary.csv\", index=False)\n",
        "        pd.DataFrame(rows_uncert).to_csv(outdir / \"phase2_uncert_per_sample.csv\", index=False)\n",
        "        pd.DataFrame(rows_uncert_summary).to_csv(outdir / \"phase2_uncert_summary.csv\", index=False)\n",
        "        logging.info(\"Run complete â€” artifacts in %s\", self.cfg.output_dir)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Part 5: main() entry point\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_logging()\n",
        "\n",
        "    # Minimal example config; replace target_column names with real CT-RATE columns\n",
        "    fields = [\n",
        "        FieldSpec(\n",
        "            name=\"pulmonary_embolism\",\n",
        "            type=\"qualitative_presence\",\n",
        "            target_column=\"pulmonary_embolism\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    models = [\n",
        "        ModelSpec(\n",
        "            name=\"llama3_8b_instruct\",\n",
        "            model_type=\"open_hf\",  # likely intended to be \"open_hf\" for Hugging Face\n",
        "            model_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "            temperature=0.0,\n",
        "            max_tokens=512\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Define your dataset paths\n",
        "    dataset_paths = [\n",
        "        \"data/ct_rate/train.csv\",\n",
        "        \"data/ct_rate/valid.csv\"\n",
        "    ]\n",
        "\n",
        "    # Example runner configuration\n",
        "    runner = ExperimentRunner(\n",
        "        fields=fields,\n",
        "        models=models,\n",
        "        dataset_paths=dataset_paths,\n",
        "        output_dir=\"results/ct_rate\",\n",
        "        verbosity=\"info\",\n",
        "        uncertainty_estimation=False\n",
        "    )\n",
        "\n",
        "    runner.run()\n",
        "\n",
        "    logging.info(\"Experiment completed successfully.\")\n"
      ],
      "metadata": {
        "id": "56FapXb7861n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "api_clients.py\n",
        "--------------\n",
        "API client wrappers for OpenAI, Fireworks, and Groq LLMs.\n",
        "\n",
        "Reads API keys from a `.env` file using `python-dotenv`.\n",
        "\n",
        "Usage:\n",
        "    from api_clients import OpenAIClient, FireworksClient, GroqClient\n",
        "    openai_client = OpenAIClient(model=\"gpt-4o-mini\")\n",
        "    resp = openai_client.generate(\"Say hello\")\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import Optional, Dict, Any, List\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env\n",
        "# .env should contain:\n",
        "#   OPENAI_API_KEY=...\n",
        "#   FIREWORKS_API_KEY=...\n",
        "#   GROQ_API_KEY=...\n",
        "load_dotenv()\n",
        "\n",
        "# Optional: install these packages if not present\n",
        "# pip install openai fireworks-ai groq python-dotenv\n",
        "\n",
        "# ----------------------------\n",
        "# Base class\n",
        "# ----------------------------\n",
        "class BaseAPIClient:\n",
        "    def __init__(self, model: str):\n",
        "        self.model = model\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        temperature: float = 0.0,\n",
        "        max_tokens: int = 512,\n",
        "        **kwargs\n",
        "    ) -> Dict[str, Any]:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# OpenAI API client\n",
        "# ----------------------------\n",
        "class OpenAIClient(BaseAPIClient):\n",
        "    def __init__(self, model: str):\n",
        "        super().__init__(model)\n",
        "        try:\n",
        "            import openai\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install the openai package: pip install openai\")\n",
        "        self.openai = openai\n",
        "        self.openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not self.openai.api_key:\n",
        "            raise ValueError(\"OPENAI_API_KEY not found in environment.\")\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        temperature: float = 0.0,\n",
        "        max_tokens: int = 512,\n",
        "        **kwargs\n",
        "    ) -> Dict[str, Any]:\n",
        "        resp = self.openai.ChatCompletion.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            **kwargs\n",
        "        )\n",
        "        content = resp.choices[0].message[\"content\"]\n",
        "        return {\"text\": content, \"raw\": resp}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Fireworks AI client\n",
        "# ----------------------------\n",
        "class FireworksClient(BaseAPIClient):\n",
        "    def __init__(self, model: str):\n",
        "        super().__init__(model)\n",
        "        try:\n",
        "            import fireworks.client\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install the fireworks-ai package: pip install fireworks-ai\")\n",
        "        self.fireworks = fireworks.client\n",
        "        self.fireworks.api_key = os.getenv(\"FIREWORKS_API_KEY\")\n",
        "        if not self.fireworks.api_key:\n",
        "            raise ValueError(\"FIREWORKS_API_KEY not found in environment.\")\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        temperature: float = 0.0,\n",
        "        max_tokens: int = 512,\n",
        "        **kwargs\n",
        "    ) -> Dict[str, Any]:\n",
        "        resp = self.fireworks.Completion.create(\n",
        "            model=self.model,\n",
        "            prompt=prompt,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            **kwargs\n",
        "        )\n",
        "        content = resp[\"choices\"][0][\"text\"]\n",
        "        return {\"text\": content, \"raw\": resp}\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Groq client\n",
        "# ----------------------------\n",
        "class GroqClient(BaseAPIClient):\n",
        "    def __init__(self, model: str):\n",
        "        super().__init__(model)\n",
        "        try:\n",
        "            from groq import Groq\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install the groq package: pip install groq\")\n",
        "        api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"GROQ_API_KEY not found in environment.\")\n",
        "        self.client = Groq(api_key=api_key)\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        temperature: float = 0.0,\n",
        "        max_tokens: int = 512,\n",
        "        **kwargs\n",
        "    ) -> Dict[str, Any]:\n",
        "        # Groq Python SDK interface\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            **kwargs\n",
        "        )\n",
        "        content = resp.choices[0].message.content\n",
        "        return {\"text\": content, \"raw\": resp}\n"
      ],
      "metadata": {
        "id": "54Yoh678_zxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ".env file\n",
        "OPENAI_API_KEY=sk-...\n",
        "FIREWORKS_API_KEY=fk-...\n",
        "GROQ_API_KEY=gk-...\n"
      ],
      "metadata": {
        "id": "M-o6LazL_9ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage in the experiment\n",
        "from api_clients import OpenAIClient, FireworksClient, GroqClient\n",
        "\n",
        "openai_client = OpenAIClient(model=\"gpt-4o-mini\")\n",
        "fw_client = FireworksClient(model=\"accounts/fireworks/models/llama-v3-70b-instruct\")\n",
        "groq_client = GroqClient(model=\"mixtral-8x7b-32768\")\n",
        "\n",
        "print(openai_client.generate(\"Hello from OpenAI\")[\"text\"])\n"
      ],
      "metadata": {
        "id": "1YJdFH3z_-4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "config_settings.py\n",
        "------------------\n",
        "Centralized configuration for experiment runs.\n",
        "Keeps dataset paths, model parameters, and other settings in one place.\n",
        "\n",
        "Import this module in your main script:\n",
        "    from config_settings import FIELDS, MODELS, DATASET_PATHS, OUTPUT_DIR\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# -----------------------\n",
        "# Project structure setup\n",
        "# -----------------------\n",
        "BASE_DIR = Path(__file__).resolve().parent\n",
        "\n",
        "# -----------------------\n",
        "# Dataset settings\n",
        "# -----------------------\n",
        "DATASET_PATHS = [\n",
        "    BASE_DIR / \"data\" / \"ct_rate\" / \"train.csv\",\n",
        "    BASE_DIR / \"data\" / \"ct_rate\" / \"valid.csv\",\n",
        "]\n",
        "\n",
        "# -----------------------\n",
        "# Output settings\n",
        "# -----------------------\n",
        "OUTPUT_DIR = BASE_DIR / \"results\" / \"ct_rate\"\n",
        "\n",
        "# -----------------------\n",
        "# Experiment parameters\n",
        "# -----------------------\n",
        "TEMPERATURE = 0.0\n",
        "MAX_TOKENS = 512\n",
        "VERBOSITY = \"info\"\n",
        "UNCERTAINTY_ESTIMATION = False\n",
        "\n",
        "# -----------------------\n",
        "# Field and model specs\n",
        "# -----------------------\n",
        "from experiment_specs import FieldSpec, ModelSpec\n",
        "# If FieldSpec & ModelSpec are in another module, update this import path accordingly\n",
        "\n",
        "FIELDS = [\n",
        "    FieldSpec(\n",
        "        name=\"pulmonary_embolism\",\n",
        "        type=\"qualitative_presence\",\n",
        "        target_column=\"pulmonary_embolism\"\n",
        "    )\n",
        "]\n",
        "\n",
        "MODELS = [\n",
        "    ModelSpec(\n",
        "        name=\"llama3_8b_instruct\",\n",
        "        model_type=\"open_hf\",  # change if using API clients: \"openai\", \"fireworks\", \"groq\"\n",
        "        model_path=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "        temperature=TEMPERATURE,\n",
        "        max_tokens=MAX_TOKENS\n",
        "    )\n",
        "]\n",
        "\n",
        "# -----------------------\n",
        "# Utility function\n",
        "# -----------------------\n",
        "def get_config_summary() -> str:\n",
        "    return (\n",
        "        f\"Datasets: {DATASET_PATHS}\\n\"\n",
        "        f\"Output dir: {OUTPUT_DIR}\\n\"\n",
        "        f\"Models: {[m.name for m in MODELS]}\\n\"\n",
        "        f\"Temperature: {TEMPERATURE}, Max tokens: {MAX_TOKENS}\\n\"\n",
        "        f\"Verbosity: {VERBOSITY}, Uncertainty: {UNCERTAINTY_ESTIMATION}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "W7Swv4n9C1lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN your Experiment\n",
        "from config_settings import FIELDS, MODELS, DATASET_PATHS, OUTPUT_DIR, VERBOSITY, UNCERTAINTY_ESTIMATION\n",
        "from utils import setup_logging\n",
        "from experiment_runner import ExperimentRunner\n",
        "import logging\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_logging()\n",
        "\n",
        "    runner = ExperimentRunner(\n",
        "        fields=FIELDS,\n",
        "        models=MODELS,\n",
        "        dataset_paths=DATASET_PATHS,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        verbosity=VERBOSITY,\n",
        "        uncertainty_estimation=UNCERTAINTY_ESTIMATION\n",
        "    )\n",
        "\n",
        "    runner.run()\n",
        "    logging.info(\"Experiment completed successfully.\")\n"
      ],
      "metadata": {
        "id": "uS9Ve_lVC943"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}